{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tqmsaTHlgL6z"
      },
      "outputs": [],
      "source": [
        "!pip install youtube_transcript_api"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from youtube_transcript_api import YouTubeTranscriptApi"
      ],
      "metadata": {
        "id": "x051OWQOgOJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_video_id(url_link):\n",
        "  return url_link.split(\"watch?v=\")[-1]"
      ],
      "metadata": {
        "id": "lUNwBRaxggu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "video_id = get_video_id(\"https://www.youtube.com/watch?v=6ixsAg_er44\")\n",
        "print(video_id)"
      ],
      "metadata": {
        "id": "vXvECRqahNMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transcript = YouTubeTranscriptApi.get_transcript(video_id)"
      ],
      "metadata": {
        "id": "59OWuzhWiM0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transcript"
      ],
      "metadata": {
        "id": "NNlDPFGfjTn2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transcript_joined = \" \".join([line['text'] for line in transcript])"
      ],
      "metadata": {
        "id": "E-u1WpjfjWNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transcript_joined"
      ],
      "metadata": {
        "id": "-Op0xNPgjd_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/babthamotharan/rpunct.git@patch-2"
      ],
      "metadata": {
        "id": "oDD82kyckISK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "id": "e6u_RBiPnTxw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from rpunct import RestorePuncts\n",
        "rpunct = RestorePuncts()"
      ],
      "metadata": {
        "id": "Ha_HFwQ5lJA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = rpunct.punctuate(transcript_joined)\n",
        "print(results)"
      ],
      "metadata": {
        "id": "Pkko0RiHox46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "model_name = \"Qwen/Qwen-1_8B-Chat\"\n",
        "hf_token = \"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\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, token=hf_token)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    token=hf_token\n",
        ")\n"
      ],
      "metadata": {
        "id": "oaO_MbBRCBSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"Summarise the following text:\\n\\n{results}\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens=300, do_sample=True, temperature=0.7)\n",
        "summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"\\n=== Summary ===\\n\")\n",
        "print(summary)"
      ],
      "metadata": {
        "id": "d3kPYmFyCywH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "GeyLMyF-IHAj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
