# -*- coding: utf-8 -*-
"""Copy of qwen-youtube.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QpS13oscDQiGmFIbjD3zLgHgdncTljK1
"""

!pip install youtube_transcript_api

from youtube_transcript_api import YouTubeTranscriptApi

def get_video_id(url_link):
  return url_link.split("watch?v=")[-1]

video_id = get_video_id("https://www.youtube.com/watch?v=vKTBBc13o88")
print(video_id)

transcript = YouTubeTranscriptApi.get_transcript(video_id)

transcript

transcript_joined = " ".join([line['text'] for line in transcript])

transcript_joined

!pip install git+https://github.com/babthamotharan/rpunct.git@patch-2

!pip install openai

from rpunct import RestorePuncts
rpunct = RestorePuncts()

results = rpunct.punctuate(transcript_joined)
print(results)

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

model_name = "Qwen/Qwen-1_8B-Chat"
hf_token = "" #input HF token

tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, token=hf_token)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",
    trust_remote_code=True,
    torch_dtype=torch.float16,
    token=hf_token
)

prompt = f"Summarise the following text:\n\n{results}"
inputs = tokenizer(prompt, return_tensors="pt").to(model.device)


outputs = model.generate(**inputs, max_new_tokens=300, do_sample=True, temperature=0.7)
summary = tokenizer.decode(outputs[0], skip_special_tokens=True)

print("\n=== Summary ===\n")
print(summary)

